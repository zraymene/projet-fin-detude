% TODO:
%    - Chapitre 2:  
%         - More detail on YOLOv4 / YOLOv5
%    - Chapitre 3:
%         - Evluation pour chaque diffuclter
%         - Conclusion general
%

\chapter{Implemnetation de détection d'objets et apprentissage profond}
\newpage
\pagestyle{fancy}
\fancyhead[L]{\chaptername \ \thechapter}
\fancyhead[R]{Implemnetation de détection d'objets et apprentissage profond}
\renewcommand{\headrulewidth}{1pt}
\fancyfoot[C]{\thepage}

\section{Introduction} 
Dans notre travail, nous allons comparer trois modèles profond de la 
famille YOLO, à savoir YOLOv3, YOLOv4, et YOLOv5, car ceux sont des détecteurs à une étape qui fournissent une détection rapide avec une grande précision qui peut fonctionner dans des machines bas de gamme comme les systèmes embarqués.

Dans ce chapitre, nous présenterons la démarche d'évaluation suivie.
Nous commençons par une étape principale de création et de préparation de la base d'entraînement. Puis, nous effectuons un entraînement des trois modèles choisis pour l'étude; YOLOv3, YOLOv4, et YOLOv5. Enfin nous faisons des tests avec des images contenant plusieurs difficultés pour évaluer la capacité de généralisation des trois modèles dans des cas d'usage  spécifiques où le contenu est différent. 

\section{Création de l'ensemble de données d'entraînement}
Les algorithmes de détection d'objets sélectionnés relèvent de l'apprentissage en profondeur. En raison de leur structure complexe,  ils doivent être entraînés à l'aide d'un ensemble de données pour atteindre l'objectif requis. L'ensemble de données affecte fortement les performances des modèles, c'est pourquoi l'ensemble de données doit être robuste pour atteindre des performances plus élevées.


Dans cette étude, nous avons choisi d'appliquer les différents modèles pour la détection des URL. Comme il n'y a aucune base de données standard pour l'évaluation et la comparaison, nous avons crée une base de données qui contient un total de 160 images contenants des URLs, (80\% (127) images pour entrainement et 20\% (33) pour l'évaluation). \\


L'URL commence par 3 lettres 'w' consécutives et un point \(www.\) suivi d'une étiquette, l'étiquette est une série de lettres anglaises de a à z (non sensible à la casse) et peut également contenir des chiffres de 0 à 9, des traits d'union peuvent être ajoutés mais pas au début ni à la fin et ajouter plus de 1 consécutivement n'est pas autorisé, La longueur du libellé est comprise entre 3 et 63 caractères maximum. Au final, après un point, une extension est ajoutée. Les extensions les plus utilisées sont : ".com", ".net" et ".org". cette partie peut être nommée le nom de domaine.

 L'URL peut commencer par un protocole tel que \(http://\) ou \(https://\) pour le HTTP sécurisé, mais les clients Web modernes comme les navigateurs ajoutent automatiquement le protocole avant l'URL s'il n'en contient pas. L'URL peut également contenir après le nom de l'extension plus de données telles que le nom de fichier \(/index.html\) ou des sous-répertoires comme \(/dir1/dir2\)\\
 
          \begin{figure}[H]
               \centering
               \includegraphics[height=1.8cm,width=14cm]{Chapitre3/img_1.png}
               \caption{Structure d'un URL.}
               \label{img3}
               \end{figure}

Pour notre base d'images, nous avons créé des noms d'étiquettes aléatoires selon les conventions précédemment répertoriées avec l'extension définie ajoutée à la fin qui est : \(.com\), \(.net\), \(.org\), \(.fr\), \(.dz\), \(.ca\), \(.uk\). ces extensions sont largement répandues spécialement dans notre région. Nous avons ajouté quelques URL contenant des données supplémentaires à la fin, mais la majorité des ensembles de données se concentrent fortement sur le nom de domaine (étiquette et extension).\\
          
Les méthodes de détection d'objet ne voient que les pixels, la lettre 'A' dans une police et diffèrent fortement d'une autre police, une lettre écrite à la main (manuscrit) peut également différer considérablement de sa contrepartie imprimée, elles ont toutes deux la même valeur sémantique mais écrites différemment (voir figure \ref{img4}). Ainsi, comme point de départ pour la création de l'ensemble de données, les URL imprimées sont utilisées et dessinées à l'aide de la police populaire \(Arial\). et pour la couleur, le noir est choisi.

          \begin{figure}[H]
               \centering
               \includegraphics[height=4cm,width=14cm]{Chapitre3/img_3.png}
               \caption{La lettre 'A' écrite par différentes polices.}
               \label{img4}
               \end{figure}

La plupart des impressions dans le monde se font sur du papier A4, dans cet esprit, l'ensemble de données contiendra des échantillons d'URL imprimés sur du papier au format A4. Les URL seront accompagnées de texte aléatoire écrit dans différentes langues : Latin, Arabe, Chinois, Russe et Indien.
          \begin{figure}[H]
               \centering
               \includegraphics[height=16cm,width=16cm]{Chapitre3/img_4.png}
               \caption{Exemple de documents générés.}
               \label{img5}
               \end{figure}

Une fois les documents prêts, il sont imprimés, mais cette étape est coûteuse et fastidieuse, surtout pour créer un grand ensemble de données. Une meilleure approche et moins coûteuse (presque 0 coût) qui est des données synthétiques, ce sont des informations (dans notre cas des images) qui sont fabriquées artificiellement plutôt que générées par le monde réel en tirant parti des algorithmes de rendu 3D photo réaliste existants.

Les bases du rendu 3D consistent à transformer des données tridimensionnelles représentées dans une série de triangles appelés maillage en écran 2D. 
Un buffer de pixels appelé Textures peut être appliqué à ce maillage pour les afficher dessus. Des algorithmes supplémentaires peuvent être utilisés pour améliorer le résultat rendu à la qualité du monde réel, comme : l'éclairage et l'ombre.

Le processus de génération de nos données synthétiques est divisé en plusieurs phases qui sont :
          \begin{figure}[H]
               \centering
               \includegraphics[height=10cm,width=16cm]{Chapitre3/img_5.png}
               \caption{Étapes des génération des données synthétiques.}
               \label{img6}
               \end{figure}

\begin{enumerate}
 \item \textbf{Création de textures: } 
 
 Après avoir créé un document dans un logiciel de traitement de texte, il doit être exporté dans une image de dimension supérieure pour 	assurer une meilleure qualité lors du rendu.\\
 
 \item \textbf{Création du maillage: } 
 
Un maillage de forme de papier A4 est créé, dans un logiciel d'infographie 3D ( Blender). 
 
 \item \textbf{Modification du maillage: } 
 
 Les modifications et les transformations sont appliquées à l'ensemble du maillage ou à un groupe de sommets dans le but de simuler un papier du monde réel. 
 
 \item \textbf{Attribuer la texture au maillage: } 
 
 La texture est chargée dans un logiciel 3D et affectée au maillage, ce qui affichera le pixel de texture au-dessus du maillage.
 
 \item \textbf{Effets supplémentaires: } 
 
 Le résultat de la phase précédente peut sembler réaliste, mais des effets supplémentaires peuvent être ajoutés pour améliorer le réalisme, tels que l'éclairage, les ombres et une image d'arrière-plan. Dans Blender, il y a plusieurs effets qui peuvent être ajoutés en utilisant des Shaders mais dans notre cas, les effets précédents sont suffisants.
 
 \item \textbf{Le Rendu: } 
 
 C'est la phase finale où l'image de sortie est produite. Les dimensions du rendu sont sélectionnées pour imiter les photos prises par un téléphone mobile.
 
 \end{enumerate} 
 
Après avoir crée de nombreuses images simulées, elles doivent être étiquetées manuellement où la boîte d'URL liée dans chaque image est définie à la main, puis enregistrées à l'aide du format d'annotation d'image YOLO où chaque image a son fichier texte d'annotation unique '.txt' nommé de la même manière que le nom de l'image . Chaque ligne du fichier d'annotation est un objet de vérité terrain dans l'image représentée comme ceci "<classe d'objet> <x> <y> <largeur> <hauteur>"
          \begin{figure}[H]
               \centering
               \includegraphics[height=8cm,width=13cm]{Chapitre3/img_6.png}
               \caption{Structure de fichier du format d'annotation YOLO.}
               \label{img7}
               \end{figure}

          Ce format de l'ensemble de données ne fonctionnera que pour les versions de YOLO écrites en Darknet (YOLOv3 et YOLOv4) et ne sera pas reconnu par YOLOv5. une plate-forme web est utilisée pour résoudre ce problème nommé Roboflow, qui héberge, annote et convertit tous les types de formats d'ensembles de données. Notre ensemble de données est chargé sur roboflow pour résoudre le problème précédent.
              
          

% ======= ENVIRMEENTS ==========
\section{Environnement expérimental}
     \subsubsection{Machine locale}Ordinateur local avec processeur Intel(R.) Core(TM) i7-8750H @ 2.20 GHz - 6 noyaux, RAM 8 GB et carte graphique GTX 1050. L'environnement local est utilisée dans la création de l'ensemble des donnes et pour l'évaluation.
     
     \subsubsection{Google Colab} Un environnement de développement intégré (IDE) basé sur le cloud, développé par Google, il offre la possibilité d’écrire, d’exécuter et de collaborer sur des notebooks Jupyter.
     
      \subsubsection{Python} C'est un langage de programmation open source, c'est le plus utilisé par les informaticiens. Ce langage est très populaire pour la gestion d'infrastructures, d'analyse de données ou dans le domaine du développement de logiciel.
      
     \subsubsection{OpenCV} C'est une bibliothèque libre, initialement développée par Intel, spécialisée dans le traitement d'images en temps réel. Dans notre travail, elle est utilisée pour lire et re-dimensionner, dessiner les boîtes englobantes et écrire les images sur le disk sur des formats compressés comme $PNG$.
     \subsubsection{Darknet} C'est un framework de réseau neuronal open source écrit en C et CUDA et prend en charge le calcul CPU et GPU. il était utilisé pour entraîner les modèles de détection d'objets YOLOv3 et YOLOv4.
     \subsubsection{PyTorch} C'est open-source gratuit et un cadre d'apprentissage automatique basé sur la bibliothèque Torch, utilisé pour des applications telles que la vision par ordinateur et le traitement du langage naturel.
     
   \subsubsection{Les outils utilisés pour la création de la base d'images}
   \begin{itemize}
   \item \textbf{Libre Office Word: }
   
   C'est un traitement de texte, il fait partie de la suite LibreOffice, une suite logicielle bureautique gratuite et open-source,  utilisée pour créer/modifier des documents et également les exporter au format PNG en tant que textures.
   
   \item \textbf{Blender: }
   
C'est un ensemble d'outils logiciels d'infographie 3D gratuits et open-source utilisé pour créer des films d'animation, des effets visuels, de l'art, des modèles imprimés en 3D, des animations graphiques, des applications 3D interactives. il est utilisé pour générer les données synthétiques. 
    
   \item \textbf{LabelImg: }
   
C'est un outil graphique d'annotation d'images. Il est écrit en Python et utilise Qt pour son interface graphique. Les annotations sont enregistrées sous forme de fichiers XML au format PASCAL VOC, le format utilisé par ImageNet. En outre, il prend également en charge les formats YOLO et CreateML. Utilisé pour étiqueter notre jeu de données et l'exporter au format YOLO.   
  
   \item \textbf{Roboflow: }
   
C'est une plate-forme de vision par ordinateur qui permet aux utilisateurs de créer des modèles de vision par ordinateur plus rapidement et avec plus de précision grâce à la fourniture de meilleures techniques de collecte de données, de prétraitement et de l'entrainement de modèles. Elle est utilisé pour résoudre le problème de conversion au format d'annotation YOLOv5.     
   \end{itemize}
   
 
% ======== TRAINING =============
\section{Entraînement des modèles}
     Les models sont entrainés sur $80\%$ (127 images) de l'ensemble des donnés. L'entraînement est réalisé dans l'environnement Google Colab.
     
 Les paramètres d'entraînement sont: \begin{itemize}
 \item  Pour \textbf{YOLOv3}, la dimension des images d'entrée est $416\times416$, $2000$ lots maximum ainsi que les paramètres trouvés dans le fichier de configuration du modèle:
 
  $classes$ est mis à 1 et $filters$ est fixés à 18 en suivant cette formule: $(n + 5) * 3$ où n est le nombre des classes. l'entrainement a duré 7 heures.
  
 \item Pour \textbf{YOLOv4},les dimensions de l'image d'entrée sont $608\times608$, 2000 lots maximum et les paramètres de configuration du modèle :

 $classes$ et $filters$ sont mises à 1 et 18 respectivement comme nous l'avons fait avec YOLOv3. l'entrainement a également duré 7 heures.
 
 \item Dans le cas de \textbf{YOLOv5}, le modèle médium est choisi car qu'il y a moins de compromis entre précision et vitesse, les dimensions par défaut de l'image d'entrée sont $640\times640$ avec 30 nombres d'époques. Contrairement aux modèles précédents, ce modèle n'a pris que 15 minutes pour terminer l'entrainement, ce qui est beaucoup très rapide.
 
 \item Le framework Darknet est utilisé pour entraîner YOLOv3 et YOLOv4, et PyTorch pour  YOLOv5. Les poids pré-entraînés officiels sont choisi pour appliquer l'apprentissage par transfert. Les poids finaux des modèles sont téléchargés sur la machine locale pour être utilisés dans la phase d'évaluation.
 \end{itemize}


% ========= TESING =============
\section{Évaluation des modèles}

\subsection{Évaluation globale}
     Les modèles sont évalué sur $20\%$ (33 images) de l'ensemble des donnes, et le seuil de $IoU$ choisi est 0.5.
     
     \textbf{YOLOv3} a fourni une précision moyenne de $70.53\%$. La précision diminue après que le rappel atteint $0.75$.
     
     \begin{figure}[H]
               \centering
               \includegraphics[height=8cm,width=15cm]{Chapitre3/yolov3_pr_Global.png}
               \caption{YOLOv3: Précision Moyenne, Courbe Précision-Rappel.}
               \label{y3_pr}
               \end{figure}

     \textbf{YOLOv4} a donné $90.91\%$ comme précision moyenne. La précision diminue après que le rappel atteint $0.98$.
     \begin{figure}[H]
               \centering
               \includegraphics[height=8cm,width=15cm]{Chapitre3/yolov4_pr_global.png}
               \caption{YOLOv4: Précision Moyenne, Courbe Précision-Rappel.}
               \label{y4_pr}
               \end{figure}

     \textbf{YOLOv5} a donné $88.63\%$ comme précision moyenne. La précision diminue après que le rappel atteint $0.91$.
     \begin{figure}[H]
               \centering
               \includegraphics[height=8cm,width=15cm]{Chapitre3/yolov5_pr_global.png}
               \caption{YOLOv5: Précision Moyenne, Courbe Précision-Rappel.}
               \label{y5_pr}
               \end{figure}
     
     %========= DIFFICULTIES ==========
\subsection{Évaluation après ajout de difficultés}

          Dans cette section, nous évaluons le modèle avec des difficultés qui n'étaient pas incluses dans l'ensemble de données d'entraînement. Ces difficultés sont :
          \paragraph{-} Différentes polices de caractères qui sont : Algerian, Bradley hand itc et Jokerman.
          \paragraph{-} Différentes couleurs de fond et couleurs de caractères.
          \paragraph{-} Rotation des images en $90^\circ$ et $180^\circ$.
          \paragraph{-} URL préfixées avec la balise de protocole \textit{https://}.
          \paragraph{-} Caractères d'URL écrits en manuscrit.
          \paragraph{-} Images avec Flou-Gaussien avec la taille du noyau: $7\times7$.
          \paragraph{-} Images avec Bruit-Gaussien.
          
          \vspace{1cm}
          
Le tableau suivant présente les performances obtenues en termes de précision moyenne (AP)fournies par les modèles pour chaque difficulté:

          \begin{table}[H]
               \begin{tabular}[2cm]{|c|c|c|c|}
                    \hline
                    \diagbox{Difficulté}{Modèle} &  YOLOv3      &   YOLOv4     &    YOLOv5 \\
                    \hline
                    Couleurs     & $31,17\%$ AP  &  $54,55\%$ AP  &  $27,27\%$ AP \\
                    \hline
                    Police(font) &  $72,73\%$ AP & $84,29\%$ AP   &  $88,07\%$ AP \\ 
                    \hline
                    Balise de protocole (\textit{https://})  & $12,12\%$ AP  &  $28,9\%$ AP   &  $13,64\%$ AP \\
                    \hline
                    Manuscript   & $0,0\%$AP    &  $0,0\%$ AP    &  $0,0\%$ AP  \\
                    \hline
                    Rotation     & 180° ($27,27\%$ AP) & 180° ($18,18\%$ AP)  & 180° ($18,18\%$ AP) \\
                              & 90°($0,0\%$ AP)     & 90°($0,0\%$ AP)      & 90°($0,0\%$ AP) \\
                    \hline
                    Flou-Gaussien ($7\times7$) & $72.73\%$ AP & $100\%$ AP & $90.91\%$ AP \\
                    \hline
                    Bruit-Gaussien & $36.36\%$ AP & $36.36\%$ AP & $83.98\%$ AP \\
                    \hline
                    \end{tabular}
               \caption{Précision moyenne de YOLOv3-YOLOv4-YOLOv5 pour différentes difficultés.}
               \end{table}
          
          % ---------- Font --------------
 \subsubsection{1- Différentes polices de caractères}
          Cette difficulté n'affecte pas beaucoup les performances des modèles. Tous les modèles avaient des précisions moyennes proches les unes des autres. YOLOv3 a la précision moyenne la plus faible ($72,7\%$). D'autre part, YOLOv5 avait le pourcentage le plus élevé ($88,07\%$) et YOLOv4 n'était pas loin de la valeur précédente avec $84,2\%$.
          \begin{figure}[H]
                   \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/font_yolov3.png}
                    \caption{Test de difficulté: Différentes polices de caractères - YOLOv3.}
                    \label{y3_t1}
                    \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/font_yolov4.png}
                    \caption{Test de difficulté: Différentes polices de caractères - YOLOv4.}
                    \label{y4_t2}
                    \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/font_yolov5.png}
                    \caption{Test de difficulté: Différentes polices de caractères - YOLOv5.}
                    \label{y5_t2}
                    \end{figure}

          % ---------- COLOR --------------
          \subsubsection{2- Différentes couleurs de fond et couleurs de caractères}
          Cette difficulté a fortement affecté les performances des modèles,
          YOLOv3 n'est pas capable de détecter les URL sur un fond coloré mais réussit à détecter les URL avec des caractères colorés, le modèle a obtenu $31,71\%$ de précision moyenne.
          YOLOv4 est le modèle performant dans cette difficulté avec une précision moyenne de $54,44\%$, il a réussi à détecter toutes les URL avec des caractères colorés et la plupart des URL avec un fond coloré.
          YOLOv5 est le moins performant de tous avec une précision moyenne de $27,27\%$ où il ne peut pas détecter complètement les URL avec des objets colorés et encore moins des arrière-plans colorés
          \begin{figure}[H]
               \centering
                \includegraphics[height=9cm,width=17cm]{Chapitre3/color_yolov3.png}
                \caption{Test de difficulté: Différentes couleurs de fond et couleurs de caractères - YOLOv3.}
                \label{y3_t3}
                \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/color_yolov4.png}
                    \caption{Test de difficulté: Différentes couleurs de fond et couleurs de caractères - YOLOv4.}
                    \label{y4_t3}
                    \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/color_yolov5.png}
                    \caption{Test de difficulté: Différentes couleurs de fond et couleurs de caractères - YOLOv5.}
                    \label{y5_t3}
                    \end{figure}
          
          % ---------- ROTATION --------------
          \subsubsection{3- Rotation des images sur $90^\circ$ et $180^\circ$}
          La rotation en $180^\circ$ n'a pas affecté les performances de tous les modèles comme cela ne s'est jamais produit. D'un autre côté, la rotation en $90\circ$ a totalement diminué les performances des modèles.
          \begin{figure}[H]
               \centering
                \includegraphics[height=12cm,width=17cm]{Chapitre3/rotation_yolov3.png}
                \caption{Test de difficulté: Rotation des images avec $90^\circ$ et $180^\circ$ - YOLOv3.}
                \label{y3_t5}
                \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=12cm,width=17cm]{Chapitre3/rotation_yolov4.png}
                    \caption{Test de difficulté: Rotation des images avec $90^\circ$ et $180^\circ$ - YOLOv4.}
                    \label{y4_t4}
                    \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=12cm,width=17cm]{Chapitre3/rotation_yolov5.png}
                    \caption{Test de difficulté: Rotation des images avec $90^\circ$ et $180^\circ$ - YOLOv5.}
                    \label{y5_t4}
                    \end{figure}

          % ---------- PROTOCOL-TAG --------------  
          \subsubsection{4- URL préfixées avec la balise de protocole \textit{https://}}
          Pour la  plupart des URL, tous les modèles n'ont pas réussi à les détecter et certains n'ont pas détecté l'URL complète (de la balise de protocole à l'extension), mais ils n'ont détecté que le nom de domaine uniquement.
          YOLOv3 a obtenu un score de $12,12\%$, YOLOv4 a fourni la précision la plus élevée avec $28,9\%$ et YOLOv5 $13,64\%$.
          \begin{figure}[H]
               \centering
                \includegraphics[height=9cm,width=17cm]{Chapitre3/https_yolov3.png}
                \caption{Test de difficulté: URL préfixées avec la balise de protocole \textit{https://} - YOLOv3.}
                \label{y3_t4}
                \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/https_yolov4.png}
                    \caption{Test de difficulté: URL préfixées avec la balise de protocole \textit{https://} - YOLOv4.}
                    \label{y4_https}
                    \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/https_yolov5.png}
                    \caption{Test de difficulté: URL préfixées avec la balise de protocole \textit{https://} - YOLOv5.}
                    \label{y5_https}
                    \end{figure}

          % ---------- MANUSCRIPT -----------
          \subsubsection{5- Caractères d'URL écrits en manuscrit}
          Tous les modèles ont échoué dans cette difficulté avec une précision moyenne de $0\%$.
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=7cm,width=17cm]{Chapitre3/test_manuscript.png}
                    \caption{Test de difficulté: Images avec Caractères d'URL écrits en manuscrit - YOLOv3 / YOLOv4 / YOLOv5.}
                    \label{test_manuscrit}
                    \end{figure}

          % ---------- FLOU ------------
          \subsubsection{6- Images avec Flou-Gaussian.}
          Le flou n'affecte pas beaucoup les performances des modèles. YOLOv3 a obtenu une précision moyenne de $72,73\%$. YOLOv4 a obtenu un score parfait de $100\%$ et YOLOv5 un score de $90,91\%$.
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/blur_yolov3.png}
                    \caption{Test de difficulté: Images avec Flou-Gaussian - YOLOv3.}
                    \label{y3_blur}
                    \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/blur_yolov4.png}
                    \caption{Test de difficulté: Images avec Flou-Gaussian - YOLOv4.}
                    \label{y4_blur}
                    \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/blur_yolov5.png}
                    \caption{Test de difficulté: Images avec Flou-Gaussian - YOLOv5.}
                    \label{y5_blur}
                    \end{figure}

          % ---------- FLOU ------------
          \subsubsection{7- Images avec Bruit-Gaussian.}
          Contrairement au flou, le bruit affecte beaucoup la précision des modèles, car les modèles détectent des faux objets.
          La précision YOLOv3 a chuté à $36,36\%$ et YOLOv4 a également chuté au même score de $36,36\%$. Contrairement aux deux, YOLOv5 a obtenu un score plus élevé de $83,98\%$.
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/bruit_yolov3.png}
                    \caption{Test de difficulté: Images avec Bruit-Gaussian - YOLOv3.}
                    \label{bruit_y3}
                    \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/bruit_yolov4.png}
                    \caption{Test de difficulté: Images avec Bruit-Gaussian - YOLOv4.}
                    \label{bruit_y4}
                    \end{figure}
          \begin{figure}[H]
                    \centering
                    \includegraphics[height=9cm,width=17cm]{Chapitre3/bruit_yolov5.png}
                    \caption{Test de difficulté: Images avec Bruit-Gaussian - YOLOv5.}
                    \label{bruit_y5}
                    \end{figure}
\section{Conlusion}

Dans ce chapitre, nous avons présenté notre méthode d'évaluation, y compris les différents tests réalisés, ainsi que les résultats obtenus et leur synthèse. Nos résultats sont exprimés en terme de précision moyenne reconnue  dans le domaine de la détection d'objets, obtenus en utilisant notre base de données créée, qui contient un total de 160 images contenants des URLs.  Nous avons démontré la capacité de généralisation des trois   modèles YOLOv3, YOLOv4 et YOLOv5.
Également, les performances des modèles sont mesurés pour des images contenant plusieurs difficultés, pour vérifier leur stabilité,  nous avons conclu que YOLOv4 est le plus robuste, et le plus stable dans ce contexte.


