{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types d'évaluation chargés:\n",
      "• 'couleur' - Nombre des images: 4\n",
      "• 'db' - Nombre des images: 3\n",
      "• 'font' - Nombre des images: 3\n",
      "• 'https' - Nombre des images: 4\n",
      "• 'manuscript' - Nombre des images: 1\n",
      "• 'rotation' - Nombre des images: 3\n",
      "Total de vrai objets: 80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "DOSSIER_EVALUATION = \"dataset_test/\"\n",
    "FICHIER_NOMS_CLASS = DOSSIER_EVALUATION + \"classes.txt\"\n",
    "\n",
    "# lire les class dans classes.txt\n",
    "classes = []\n",
    "fichier_noms_class = open(FICHIER_NOMS_CLASS, \"r\")\n",
    "while True:\n",
    "     line = fichier_noms_class.readline()\n",
    "     if not line: \n",
    "          break\n",
    "     classes.append(line.strip()) # lire sans '\\n'\n",
    "\n",
    "g_vrai_objets_total = 0\n",
    "\n",
    "types_evaluations = []\n",
    "img_buffer = []\n",
    "img_annotation = []\n",
    "evaluations_nombre_vrai_objet = [] # contient nombre de vrai objet pour chaque evaluation\n",
    "\n",
    "# format de nom d'image: <type>_<nombre>.png/jpg/jpeg\n",
    "# check image a un fichier d'annotation \".txt\" sur la format YOLO\n",
    "for fichier in os.listdir(DOSSIER_EVALUATION):\n",
    "     \n",
    "     # n'autoriser que les images\n",
    "     string_couper = fichier.split('.') \n",
    "     extension = string_couper[1]\n",
    "     if extension != \"png\" and extension != \"jpg\" and extension != \"jpeg\" and extension != \"bmp\": \n",
    "          continue\n",
    "     \n",
    "     # vérifier le type d'évaluation, ajouter si n'existe pas\n",
    "     current_evaluation_index = -1\n",
    "     nom_evaluation = string_couper[0].split('_')[0]\n",
    "     evaluation_trouver = False\n",
    "     for type_index, type in enumerate(types_evaluations):\n",
    "          current_evaluation_index = type_index\n",
    "          if(type == nom_evaluation):\n",
    "               evaluation_trouver = True\n",
    "               break\n",
    "\n",
    "     if not evaluation_trouver:\n",
    "          types_evaluations.append(nom_evaluation)\n",
    "          img_buffer.append([])\n",
    "          img_annotation.append([])\n",
    "          evaluations_nombre_vrai_objet.append(0)\n",
    "          current_evaluation_index += 1\n",
    "     \n",
    "     # lire l'image\n",
    "     #img_buffer[current_evaluation_index].append([])\n",
    "     img = cv2.imread(DOSSIER_EVALUATION + fichier)\n",
    "     hauteur, largeur,_ = img.shape\n",
    "     img_buffer[current_evaluation_index].append(img)\n",
    "     \n",
    "     \n",
    "     # lire le fichier d'annotation \n",
    "     img_annotation[current_evaluation_index].append([])\n",
    "     fichier_annotation = open(DOSSIER_EVALUATION + string_couper[0] + \".txt\", \"r\")\n",
    "     while True:\n",
    "          line = fichier_annotation.readline().strip()\n",
    "          if not line:\n",
    "               break\n",
    "          # decoder YOLO format\n",
    "          data = line.split(' ')\n",
    "          data[0] = int(data[0])\n",
    "          data[1] = float(data[1]) * largeur\n",
    "          data[2] = float(data[2]) * hauteur\n",
    "          data[3] = float(data[3]) * largeur \n",
    "          data[4] = float(data[4]) * hauteur\n",
    "\n",
    "          # faire le point en haut à gauche au lieu du centre\n",
    "          data[1] -= data[3] / 2\n",
    "          data[2] -= data[4] / 2\n",
    "\n",
    "          img_annotation[current_evaluation_index][-1].append(data)\n",
    "          evaluations_nombre_vrai_objet[current_evaluation_index] += 1\n",
    "          g_vrai_objets_total += 1\n",
    "\n",
    "# résumé des données de test\n",
    "print(\"Types d'évaluation chargés:\")\n",
    "for id, evaluation in enumerate(types_evaluations):\n",
    "     print(\"• '\" + evaluation + \"' - Nombre des images: \" + str(len(img_annotation[id])))\n",
    "\n",
    "print(\"Total de vrai objets: \" + str(g_vrai_objets_total))\n",
    "\n",
    "# Fonctions utilitaires\n",
    "def IntersectionSurUnion(x1, y1, largeur1, hauteur1, \\\n",
    "                         x2, y2, largeur2, hauteur2):\n",
    "\n",
    "     x1_droit = x1 + largeur1\n",
    "     y1_droit = y1 + hauteur1\n",
    "\n",
    "     x2_droit = x2 + largeur2\n",
    "     y2_droit = y2 + hauteur2\n",
    "\n",
    "     dx = min(x1_droit, x2_droit) - max(x1, x2)\n",
    "     dy = min(y1_droit, y2_droit) - max(y1, y2)\n",
    "\n",
    "     # vérifier s'ils se croisent\n",
    "     if dx <= 0 or dy <= 0:\n",
    "          return 0\n",
    "\n",
    "     # surface de chevauchement\n",
    "     surface_chevauchement = dx * dy\n",
    "\n",
    "     # surface d'union\n",
    "     surface_union = ( (largeur1 * hauteur1) + (largeur2 * hauteur2) - surface_chevauchement )\n",
    "\n",
    "     return surface_chevauchement / surface_union\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-1-31 Python-3.9.0 torch-1.13.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• 'couleur' => Précision: 0.75 - Rappel: 0.2\n",
      "• 'db' => Précision: 1.0 - Rappel: 1.0\n",
      "• 'font' => Précision: 0.7692307692307693 - Rappel: 0.8333333333333334\n",
      "• 'https' => Précision: 0.5 - Rappel: 0.125\n",
      "• 'manuscript' => Précision: 0 - Rappel: 0.0\n",
      "• 'rotation' => Précision: 1.0 - Rappel: 0.08333333333333333\n",
      "# Global => Précision: 1.0 - Rappel: 0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "# YOLOv5\n",
    "import torch\n",
    "\n",
    "DIMENSIONS_ENTREE = 640\n",
    "\n",
    "DOSSIER_SORTIE = \"predictions/yolov5/\"\n",
    "FICHIER_POIDS = \"weights/yolov5_best.pt\"\n",
    "\n",
    "CONF_SEUIL = 0.5\n",
    "IOU_SEUIL = 0.5\n",
    "\n",
    "yolov5 = torch.hub.load('ultralytics/yolov5', 'custom', path=FICHIER_POIDS)\n",
    "yolov5.conf = CONF_SEUIL\n",
    "\n",
    "# TP: IoU >= THR\n",
    "# FP: IoU < THR && Incorrect detection\n",
    "# Précision : TP / (TP + FP) = TP / Tous les prediction\n",
    "# Rappel: TP / (TP + FN) = TP / Tous les vrai objets\n",
    "# Tous les prediction: retourné par le model\n",
    "# Tous les vrai objets: nombre des annotation dans un image\n",
    "# Il rest TP utilisant le seuillage ou IoU >= SEUIL\n",
    "\n",
    "g_tp = 0\n",
    "g_prediction_total = 0\n",
    "\n",
    "for eve_id, evaluation in enumerate(types_evaluations):\n",
    "     eve_tp = 0\n",
    "     eve_prediction_total = 0\n",
    "     eve_vrai_objets = evaluations_nombre_vrai_objet[eve_id]\n",
    "\n",
    "     # prepare l'image: RGB, 640x640\n",
    "     for img_id, img in enumerate(img_buffer[eve_id]):\n",
    "          input = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "          input = cv2.resize(input, (DIMENSIONS_ENTREE, DIMENSIONS_ENTREE), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "          prediction = yolov5([input])\n",
    "          detections = prediction.xyxy[0].detach().cpu().numpy()\n",
    "\n",
    "          hauteur, largeur,_ = img.shape\n",
    "          echelle_largeur = DIMENSIONS_ENTREE / largeur \n",
    "          echelle_hateur = DIMENSIONS_ENTREE / hauteur\n",
    "\n",
    "          # d: [ xmin, ymin, xmax, ymax, confd, class ]\n",
    "          for d in detections:\n",
    "               g_prediction_total += 1\n",
    "               eve_prediction_total += 1\n",
    "\n",
    "               # a: [ class, xmin, ymin, largeur, hatueur ]\n",
    "               for a in img_annotation[eve_id][img_id]:\n",
    "                    detection_largeur = d[2] - d[0]\n",
    "                    detection_hateur = d[3] - d[1]\n",
    "\n",
    "                    # dimension à l'échelle 640x640\n",
    "                    vrai_x = a[1] * echelle_largeur\n",
    "                    vrai_y = a[2] * echelle_hateur\n",
    "                    vrai_largeur = a[3] * echelle_largeur\n",
    "                    vrai_hauteur = a[4] * echelle_hateur\n",
    "\n",
    "                    iou_score = IntersectionSurUnion(d[0], d[1], detection_largeur, detection_hateur, \n",
    "                                                       vrai_x, vrai_y, vrai_largeur, vrai_hauteur)\n",
    " \n",
    "                    # IoU >= seuil et class correct\n",
    "                    if iou_score >= IOU_SEUIL and d[5] == a[0]:\n",
    "                         g_tp += 1\n",
    "                         eve_tp += 1\n",
    "\n",
    "          # Dessinez des boîtes, basculez les canaux de couleur vers BGR puis écrivez sur le disque\n",
    "          result = prediction.render()[0]\n",
    "          result = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)\n",
    "          cv2.imwrite(DOSSIER_SORTIE + types_evaluations[eve_id] + \"_\" + str(img_id) + \".PNG\", result)\n",
    "\n",
    "     # Calculate Precision et rappel pour l'evaluation current\n",
    "     eve_precision = (eve_tp / eve_prediction_total) if eve_prediction_total != 0 else 0\n",
    "     eve_rappel =  eve_tp / eve_vrai_objets\n",
    "\n",
    "     print(\"• '\" + types_evaluations[eve_id] + \"' => Précision: \" + str(eve_precision) + \" - Rappel: \" + str(eve_rappel))\n",
    "\n",
    "# Global\n",
    "g_precision = g_tp / g_prediction_total\n",
    "g_rappel =  g_tp / g_vrai_objets_total\n",
    "\n",
    "print(\"# Global => Précision: \" + str(eve_precision) + \" - Rappel: \" + str(eve_rappel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• 'couleur' => Précision: 1.0 - Rappel: 0.5333333333333333\n",
      "• 'db' => Précision: 1.0 - Rappel: 1.0\n",
      "• 'font' => Précision: 0.875 - Rappel: 0.5833333333333334\n",
      "• 'https' => Précision: 0.75 - Rappel: 0.1875\n",
      "• 'manuscript' => Précision: 0 - Rappel: 0.0\n",
      "• 'rotation' => Précision: 1.0 - Rappel: 0.16666666666666666\n",
      "# Global => Précision: 1.0 - Rappel: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "# YOLOv4\n",
    "import numpy as np\n",
    "\n",
    "DOSSIER_SORTIE = \"predictions/yolov4/\"\n",
    "\n",
    "# en raison des limites du fichier dans Github, le poids YOLOv4 est téléchargeable depuis Google Drive:\n",
    "# https://drive.google.com/file/d/1-BhVjp-AaIMeWKC0veV_WWac2zYzX4Ye/view?usp=sharing\n",
    "FICHIER_POIDS = \"weights/yolov4_best.weights\"\n",
    "FICHIER_CFG = \"weights/yolov4.cfg\"\n",
    "\n",
    "DIMENSIONS_ENTREE = 608\n",
    "\n",
    "CONF_SEUIL = 0.5\n",
    "IOU_SEUIL = 0.5\n",
    "\n",
    "yolov4 = cv2.dnn.readNet(FICHIER_POIDS, FICHIER_CFG)\n",
    "\n",
    "g_tp = 0\n",
    "g_prediction_total = 0\n",
    "\n",
    "for eve_id, evaluation in enumerate(types_evaluations):\n",
    "     eve_tp = 0\n",
    "     eve_prediction_total = 0\n",
    "     eve_vrai_objets = evaluations_nombre_vrai_objet[eve_id]\n",
    "\n",
    "     # prepare l'image: RGB 416x416\n",
    "     for img_id, img in enumerate(img_buffer[eve_id]):\n",
    "          hauteur, largeur,_ = img.shape\n",
    "          echelle_largeur = 640 / largeur \n",
    "          echelle_hateur = 640 / hauteur\n",
    "          \n",
    "          blob = cv2.dnn.blobFromImage(img, 1/255, (DIMENSIONS_ENTREE, DIMENSIONS_ENTREE), (0,0,0), swapRB=True, crop=False)\n",
    "          yolov4.setInput(blob)\n",
    "          output_layers_names = yolov4.getUnconnectedOutLayersNames()\n",
    "          layerOutputs = yolov4.forward(output_layers_names)\n",
    "\n",
    "          boxes = []\n",
    "          confidences = []\n",
    "          class_ids = []\n",
    "\n",
    "          for output in layerOutputs:\n",
    "              for detection in output:\n",
    "                  scores = detection[5:]\n",
    "                  class_id = np.argmax(scores)\n",
    "                  confidence = scores[class_id]\n",
    "                  if confidence > CONF_SEUIL:\n",
    "                      center_x = int(detection[0] * largeur)\n",
    "                      center_y = int(detection[1]* hauteur)\n",
    "                      w = int(detection[2]* largeur)\n",
    "                      h = int(detection[3]* hauteur)\n",
    "                      x = int(center_x - w/2)\n",
    "                      y = int(center_y - h/2)\n",
    "\n",
    "                      boxes.append([x, y, w, h])\n",
    "                      confidences.append((float(confidence)))\n",
    "                      class_ids.append(class_id)\n",
    "\n",
    "          indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.2, 0.4)\n",
    "          \n",
    "          if len(indexes)>0:\n",
    "               for i in indexes.flatten():\n",
    "                    g_prediction_total += 1\n",
    "                    eve_prediction_total += 1\n",
    "                    \n",
    "                    x, y, w, h = boxes[i]\n",
    "                    label = str(classes[class_ids[i]])\n",
    "                    confidence = str(round(confidences[i],2))\n",
    "\n",
    "                    cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "                    cv2.putText(img, label + \" \" + confidence, (x, y+20), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,255), 2)\n",
    "\n",
    "                    cv2.imwrite(DOSSIER_SORTIE + types_evaluations[eve_id] + \"_\" + str(img_id) + \".PNG\", img)\n",
    "\n",
    "                    for a in img_annotation[eve_id][img_id]:\n",
    "                         detection_largeur = d[2] - d[0]\n",
    "                         detection_hateur = d[3] - d[1]\n",
    "\n",
    "                         # dimension à l'échelle 640x640\n",
    "                         vrai_x = a[1] * echelle_largeur\n",
    "                         vrai_y = a[2] * echelle_hateur\n",
    "                         vrai_largeur = a[3] * echelle_largeur\n",
    "                         vrai_hauteur = a[4] * echelle_hateur\n",
    "\n",
    "                         iou_score = IntersectionSurUnion(x, y, w, h, \n",
    "                                                            a[1], a[2], a[3], a[4])\n",
    "     \n",
    "                         # IoU >= seuil et class correct\n",
    "                         if iou_score >= IOU_SEUIL and class_ids[i] == a[0]:\n",
    "                              g_tp += 1\n",
    "                              eve_tp += 1\n",
    "\n",
    "          # ecrivez dans le disk\n",
    "          cv2.imwrite(DOSSIER_SORTIE + types_evaluations[eve_id] + \"_\" + str(img_id) + \".PNG\", img)\n",
    "\n",
    "     # Calculate Precision et rappel pour l'evaluation current\n",
    "     eve_precision = (eve_tp / eve_prediction_total) if eve_prediction_total != 0 else 0\n",
    "     eve_rappel =  eve_tp / eve_vrai_objets\n",
    "\n",
    "     print(\"• '\" + types_evaluations[eve_id] + \"' => Précision: \" + str(eve_precision) + \" - Rappel: \" + str(eve_rappel))\n",
    "\n",
    "# Global\n",
    "g_precision = g_tp / g_prediction_total\n",
    "g_rappel =  g_tp / g_vrai_objets_total\n",
    "\n",
    "print(\"# Global => Précision: \" + str(eve_precision) + \" - Rappel: \" + str(eve_rappel))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
